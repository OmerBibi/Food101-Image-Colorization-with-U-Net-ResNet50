# ğŸ” Food Colorization using Deep Networks: Grayscale to Vibrant Color
### *Advanced Deep Learning Colorization using U-Net & ResNet-50*

<p align="center">
  <img src="outputs/colorized_video.gif" alt="Colorized Food Commercial" width="1200" />
</p>

<p align="center"> 
  <i>Video generated by Google's Nano Banana, and inferenced by our model</i> <br> <br>
Final project for the Technion's EE Deep Learning course (046217)
<p align="center"> 
  
---

# âš ï¸ **Important**

This repository uses **Git LFS** for model checkpoints.
If you skip Git LFS, inference will silently break.

 Install Git LFS: https://git-lfs.com  

## ğŸ“‘ Table of Contents

- [ğŸ“– Introduction](#-introduction)
- [ğŸ› ï¸ Model Architecture & Methodology](#ï¸-model-architecture--methodology)
- [ğŸ“ˆ Training Summary](#-training-summary)
- [ğŸ–¼ï¸ Results Gallery](#ï¸-results-gallery)
- [ğŸ“‚ Repository Contents](#-repository-contents)
- [ğŸ’» Setup & Usage](#-setup--usage)

---

## ğŸ“– Introduction
This project implements a colorization pipeline using a **U-Net** architecture with a **ResNet-50** backbone, trained on the **Food-101** dataset. By leveraging the CIELAB color space, the model learns to predict the $a$ and $b$ (color) channels given the $L$ (luminance) channel.

### ğŸ¯ Project Highlights
* **High-Fidelity Colorization**: Uses annealed mean decoding ($T=0.42$) for vibrant results.
* **Video Capability**: Successfully processes video sequences frame-by-frame.
* **Smart Architecture**: Combines pre-trained ResNet-50 features with U-Net skip connections.

---

## ğŸ› ï¸ Model Architecture & Methodology

The core of this project is a **Quantized Color Class Prediction** model. Instead of predicting a single color value (which leads to "muddy" gray results), the model predicts a probability distribution over 259 discrete color bins.

| Stage | Description |
| :--- | :--- |
| **Preprocessing** | Lab color space conversion, color quantization, and rare-class rebalancing. |
| **Encoder** | Modified **ResNet-50** (accepting 1-channel grayscale input). |
| **Decoder** | Symmetric upsampling path with **Skip Connections** to preserve edges. |
| **Inference** | Annealed Mean decoding to balance color saturation and realism. |
<p align="center">
<img width="444" height="608" alt="graphviz (1)" src="https://github.com/user-attachments/assets/a8545a93-3179-4076-b949-c9a55e5f22d0" />
<p align="center">
  

### âœ Key Design Choices

#### Why classification instead of regression
Image colorization is inherently **multimodal**. The same grayscale texture can map to many valid colors
(for example, red vs green apples, or rare vs well-done meat).  
Direct regression with an L2 loss forces the model to average these possibilities, which leads to
desaturated, brownish results.

Instead, the problem is framed as **classification over quantized ab color bins**.
The network predicts a probability distribution over colors for each pixel,
allowing it to represent multiple plausible outcomes rather than a single mean value.

This formulation is critical for producing vibrant and realistic colors.


#### Why CIE Lab color space
The CIE Lab color space separates **luminance (L)** from **chrominance (a, b)**.

- The input grayscale image provides the L channel directly.
- The network only needs to predict the ab channels.

This separation simplifies the learning problem:
the model does not waste capacity on brightness reconstruction and can focus entirely on color.
Lab space also aligns better with human perception, making color errors less visually disturbing.



#### Why annealed mean decoding (T = 0.42)
During inference, directly taking the argmax color bin produces noisy, pixel-level artifacts.
A standard mean, on the other hand, pushes predictions toward gray.

Annealed mean decoding applies a temperature-scaled softmax before computing the expected color:
- **Lower temperature** sharpens the distribution.
- **Higher temperature** smooths it.

A temperature of **T = 0.42** was chosen empirically to balance
color vividness and spatial consistency, producing strong colors without speckle noise.



#### Why class rebalancing
Natural images contain far more neutral colors than saturated ones.
Without correction, the network learns to overpredict gray and brown tones.

To counter this imbalance, class-frequency-based weights are applied in the loss function.
Rare but important colors receive higher weight, encouraging the model to use the full color space.

This step is essential for avoiding color collapse and achieving visually pleasing results.



---

## ğŸ“ˆ Training Summary

Training converges smoothly with stable optimization. While validation loss increases at later epochs, perceptual quality (LPIPS) and image fidelity (SSIM, PSNR) continue to improve and stabilize. We therefore use the epoch 39 checkpoint, which provides the best visual and perceptual results in practice.

<p align="center">
  <img src="artifacts/food101_step10_sigma5_T042/train_runs/long_run_45/training_curves.png" alt="Colorized Food Commercial" width="600" />
</p>

---

## ğŸ–¼ï¸ Results Gallery

### Side-by-Side Test Comparisons
Below are several examples from the test set showing the input grayscale, our model's prediction, and the ground truth.

<p align="center">
  <table>
    <tr>
      <td><img src="outputs/colorization_comparison_1.png" width="450" alt="Evaluation 1"></td>
      <td><img src="outputs/colorization_comparison_2.png" width="450" alt="Evaluation 2"></td>
    </tr>
  </table>
</p>

---

## ğŸ“‚ Repository Contents

### Source Code (`src/`)

Core implementation of the colorization pipeline.

* **`data/`** â€“ Dataset handling and preprocessing  
  * `datasets.py` â€“ Food-101 colorization dataset with soft-encoding of ab bins  
  * `transforms.py` â€“ Custom image transforms (Lab conversion, resizing, normalization)  
  * `loaders.py` â€“ DataLoader construction and train/val splits  

* **`models/`** â€“ Model architectures and losses  
  * `unet_resnet50.py` â€“ U-Net decoder with ResNet-50 encoder (1-channel input)  
  * `blocks.py` â€“ Reusable convolutional and upsampling blocks  
  * `losses.py` â€“ Classification loss with class rebalancing  

* **`training/`** â€“ Training infrastructure  
  * `trainer.py` â€“ Training and validation loops  
  * `checkpoint.py` â€“ Checkpoint save / load logic  
  * `logger.py` â€“ Metric and progress logging utilities  

* **`utils/`** â€“ Shared utilities  
  * `color_utils.py` â€“ Lab â†” RGB conversions, annealed mean decoding  
  * `visualization.py` â€“ Inference helpers and plotting utilities  
  * `metrics.py` â€“ Training-time evaluation metrics  
  * `config.py` â€“ Config loading and validation  

> Note: Some utility files may not be imported directly by entry scripts but are used
internally by training, inference, or visualization pipelines.

### Scripts (`scripts/`)
* `train.py` - Main training entry point

### Configuration (`configs/`)
* `default.yaml` - Base configuration with model architecture and hyperparameters

### Notebooks
* `visualization.ipynb` - Inference and visualization tools

---

## ğŸ’» Setup & Usage
### ğŸ“Œ Overview
This repository implements image colorization on **Food-101** using a **U-Net decoder with a ResNet50 encoder**, trained in LAB color space with **soft-encoded ab bins**.

ğŸ“‚ Main components:
- `src/` â€“ Modular source code (data, models, training, utils)
- `scripts/train.py` â€“ Training entry point
- `configs/` â€“ YAML configuration files

### ğŸ“¥ 1) Clone the repository
```bash
git clone https://github.com/OmerBibi/Food101-Image-Colorization-with-U-Net-ResNet50
cd Food101-Image-Colorization-with-U-Net-ResNet50
```
âš ï¸ Important: model weights use Git LFS

This repository stores trained model weights using Git LFS.
If you don't have Git LFS installed, the weights will not be downloaded correctly.

Option A: install Git LFS (recommended), then clone the repo as usual.

If the weights files look very small (a few KB), try running 
```bash
git lfs pull
```
If the problem persists, Git LFS is not set up correctly.

Option B: download weights manually through your browser.

We recommend using `artifacts/food101_step10_sigma5_T042/train_runs/long_run_45/checkpoints/lpips/best_ep039_lpips0.1575.pt`


### ğŸ 2) Environment setup (recommended)
```bash
conda create -n foodcolor python=3.8 -y
conda activate foodcolor
```

### ğŸ“¦ 3) Install dependencies
```bash
pip install -e .
```
âš ï¸ Notes:
* For GPU training, install a CUDA-enabled PyTorch build matching your CUDA version.
### ğŸ—‚ï¸ 4) Folder structure (important)
Top-level layout in this repo:

```text
Food101-Image-Colorization-with-U-Net-ResNet50/
â”œâ”€ src/                    # Core package (data, models, training, utils)
â”œâ”€ scripts/                # Entry points (training, etc.)
â”œâ”€ configs/                # YAML configs
â”œâ”€ data/                   # Downloaded dataset + cached assets (created at runtime)
â”œâ”€ artifacts/              # Precomputed color bins + trained checkpoints (tracked via Git LFS)
â”œâ”€ outputs/                # Result images / gifs for the README
â”œâ”€ reference/              # Reference figures / notes
â”œâ”€ visualization.ipynb     # Inference + visualization notebook
â”œâ”€ requirements.txt
â””â”€ setup.py
```
âœ… Make sure the `artifacts/food101_step10_sigma5_T042/` folder exists and contains the required .npy files and checkpoints.
Required artifacts (used by training + inference):
```text
artifacts/
â””â”€ food101_step10_sigma5_T042/
   â”œâ”€ ab_centers_k259.npy
   â”œâ”€ ab_weights_k259.npy
   â””â”€ train_runs/
      â””â”€ .../checkpoints/
         â””â”€ best_*.pt
```

### ğŸš€ 5) Training
To train or retrain the model:
```bash
python scripts/train.py
```

With custom configuration:
```bash
python scripts/train.py --config configs/default.yaml
```

What happens:
* Food-101 will be downloaded into the data/ directory
* Food-101 is split into train/validation
* RGB images are converted to LAB
* ab channels are soft-encoded using KNN
* Encoder is frozen for warmup, then unfrozen
* Best checkpoints are saved automatically
* Visual diagnostics are written to:
  `train_runs/.../viz/` , `train_runs/.../strips/consistency_filmstrip.png`

### âš™ï¸ 6) Configuration
Modify `configs/default.yaml` to adjust:
* **Training hyperparameters**: batch size, learning rate, epochs
* **Model architecture**: encoder (resnet18/34/50/101), decoder channels, skip connections
* **Paths and directories**: data location, output paths

Example configuration sections:
```yaml
model:
  encoder: "resnet50"              # Options: resnet18, resnet34, resnet50, resnet101
  decoder_channels: [1024, 512, 256, 128, 64]
  skip_connections: true

training:
  batch_size: 64
  epochs: 45
  lr_decoder: 0.001
  lr_encoder: 0.0001
```

### ğŸ¨ 7) Inference and visualization
Use `visualization.ipynb`

ğŸ›ï¸Important inference parameter:

`ANNEAL_T = 0.42`
Lower values give sharper colors but may introduce artifacts.

### ğŸ› ï¸ 8) Customization
You can modify:

- âš™ï¸ **Training hyperparameters** in `configs/default.yaml`
  - Batch size, learning rates, epochs, weight decay
- ğŸ—ï¸ **Model architecture** in `configs/default.yaml`
  - Encoder backbone (ResNet18/34/50/101)
  - Decoder channel dimensions
  - Skip connections on/off
- ğŸ§ª **Preprocessing logic**
- ğŸ”¢ **Number of ab bins (K)**
  - âš ï¸ Changing K requires regenerating centers and weights and retraining

### â— 9) Common issues
- ğŸ“ Missing artifacts - Check folder name: food101_step10_sigma5_T042

- ğŸ§¯ CUDA out-of-memory - Reduce batch size

- ğŸ­ Desaturated or unstable colors - Tune ANNEAL_T during inference
  
---
*Inspired by the "Colorful Image Colorization" paper (Zhang et al.) and built for the Food-101 Challenge.*
