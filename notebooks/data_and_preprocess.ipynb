{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1: Imports + global config\n",
    "# =========================\n",
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Food101\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from skimage.color import rgb2lab\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Artifacts folder\n",
    "ART_DIR = Path(\"artifacts/food101_step10_sigma5_T042\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K = 300\n",
    "# Locked choices\n",
    "AB_MIN, AB_MAX = -110.0, 110.0\n",
    "GRID_STEP = 10.0\n",
    "\n",
    "SOFT_KNN = 5\n",
    "SIGMA_SOFT = 5.0       # target soft-encoding sigma\n",
    "SIGMA_SMOOTH = 5.0     # prior smoothing sigma\n",
    "LAMBDA_UNIFORM = 0.5\n",
    "\n",
    "ANNEAL_T = 0.42\n",
    "\n",
    "# Bin-building + prior settings\n",
    "PRUNE_IMAGES = 30_000         # grid prune uses 30K images\n",
    "PRIOR_USE_ALL_TRAIN = True    # use all train images for prior\n",
    "PRIOR_BATCH_IMAGES = 256       # for prior estimation loop (image-level batches)\n",
    "\n",
    "# Training settings\n",
    "NUM_WORKERS = 6\n",
    "EPOCHS = 15\n",
    "LR_DECODER = 1e-3\n",
    "LR_ENCODER = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP = 1.0\n",
    "FREEZE_EPOCHS = 1  # epoch 0..FREEZE_EPOCHS-1 frozen encoder\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Artifacts:\", ART_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6769a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2: Resize policy A transforms (torchvision)\n",
    "#   - Train: Resize short side 256, RandomCrop 224, optional flip\n",
    "#   - Val/Test/Prior: Resize short side 256, CenterCrop 224\n",
    "# =========================\n",
    "\n",
    "class ResizeShortSide:\n",
    "    def __init__(self, short_side: int, interpolation=Image.BICUBIC):\n",
    "        self.short_side = short_side\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if min(w, h) == self.short_side:\n",
    "            return img\n",
    "        scale = self.short_side / float(min(w, h))\n",
    "        new_w = int(round(w * scale))\n",
    "        new_h = int(round(h * scale))\n",
    "        return img.resize((new_w, new_h), self.interpolation)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    ResizeShortSide(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    ResizeShortSide(256),\n",
    "    transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "# For bin pruning and priors we want center crop stability\n",
    "prior_tf = eval_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d43e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: Food101 datasets + train/val split\n",
    "# =========================\n",
    "DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "train_base = Food101(root=str(DATA_ROOT), split=\"train\", download=True)\n",
    "test_base  = Food101(root=str(DATA_ROOT), split=\"test\",  download=True)\n",
    "\n",
    "# Deterministic val split from train\n",
    "val_frac = 0.1\n",
    "n_train = len(train_base)\n",
    "idx = np.arange(n_train)\n",
    "rng = np.random.default_rng(SEED)\n",
    "rng.shuffle(idx)\n",
    "n_val = int(round(val_frac * n_train))\n",
    "val_idx = idx[:n_val].tolist()\n",
    "trn_idx = idx[n_val:].tolist()\n",
    "\n",
    "print(\"Food101 sizes:\")\n",
    "print(\" train:\", len(train_base), \" -> trn:\", len(trn_idx), \" val:\", len(val_idx))\n",
    "print(\" test :\", len(test_base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4: LAB helpers (CPU)\n",
    "# =========================\n",
    "def pil_to_rgb01(img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"PIL RGB -> float32 [0,1] HxWx3\"\"\"\n",
    "    arr = np.asarray(img.convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "    return arr\n",
    "\n",
    "def rgb01_to_lab(rgb01: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"rgb01 HxWx3 -> LAB float32 HxWx3 (L in [0,100], ab ~ [-128,127])\"\"\"\n",
    "    lab = rgb2lab(rgb01).astype(np.float32)\n",
    "    return lab\n",
    "\n",
    "def clamp_ab(lab: np.ndarray, ab_min=AB_MIN, ab_max=AB_MAX) -> np.ndarray:\n",
    "    lab = lab.copy()\n",
    "    lab[..., 1] = np.clip(lab[..., 1], ab_min, ab_max)\n",
    "    lab[..., 2] = np.clip(lab[..., 2], ab_min, ab_max)\n",
    "    return lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5: grid + pruning (nearest grid point), weighted k-means to min(K=300 , observed effective bins) \n",
    "# =========================\n",
    "CENTERS_NPY = ART_DIR / f\"ab_centers_k{K}.npy\"\n",
    "\n",
    "def build_ab_grid(step=GRID_STEP, ab_min=AB_MIN, ab_max=AB_MAX):\n",
    "    vals = np.arange(ab_min, ab_max + 1e-6, step, dtype=np.float32)\n",
    "    aa, bb = np.meshgrid(vals, vals, indexing=\"xy\")\n",
    "    grid = np.stack([aa.reshape(-1), bb.reshape(-1)], axis=1)  # (G,2)\n",
    "    return grid, vals\n",
    "\n",
    "GRID_POINTS, GRID_VALS = build_ab_grid()\n",
    "G = GRID_POINTS.shape[0]\n",
    "print(\"Grid points:\", G)\n",
    "\n",
    "# Nearest grid-point snapping:\n",
    "# since grid is regular, nearest snapping is just round((x - ab_min)/step)*step + ab_min\n",
    "def snap_to_grid(ab: np.ndarray, step=GRID_STEP, ab_min=AB_MIN, ab_max=AB_MAX):\n",
    "    x = np.clip(ab, ab_min, ab_max)\n",
    "    snapped = np.round((x - ab_min) / step) * step + ab_min\n",
    "    snapped = np.clip(snapped, ab_min, ab_max)\n",
    "    return snapped.astype(np.float32)\n",
    "\n",
    "def grid_index(snapped_ab: np.ndarray, step=GRID_STEP, ab_min=AB_MIN, ab_max=AB_MAX):\n",
    "    # map (a,b) on grid to a unique index in the flattened grid\n",
    "    # values are in GRID_VALS, so index is computed by integer coordinates\n",
    "    coord = np.round((snapped_ab - ab_min) / step).astype(np.int32)\n",
    "    size = int(round((ab_max - ab_min) / step)) + 1  # e.g. 23\n",
    "    a_i = coord[:, 0]\n",
    "    b_i = coord[:, 1]\n",
    "    return b_i * size + a_i  # consistent with meshgrid flattening\n",
    "\n",
    "def build_centers_from_food101():\n",
    "    if CENTERS_NPY.exists():\n",
    "        print(\"Centers already exist:\", CENTERS_NPY)\n",
    "        return np.load(CENTERS_NPY).astype(np.float32)\n",
    "\n",
    "    # select PRUNE_IMAGES from train split\n",
    "    prune_n = min(PRUNE_IMAGES, len(trn_idx))\n",
    "    prune_ids = trn_idx[:prune_n]\n",
    "\n",
    "    counts = np.zeros(G, dtype=np.int64)\n",
    "\n",
    "    for j, i in enumerate(prune_ids, 1):\n",
    "        img, _ = train_base[i]\n",
    "        img = prior_tf(img)  # center crop\n",
    "        rgb01 = pil_to_rgb01(img)\n",
    "        lab = clamp_ab(rgb01_to_lab(rgb01))\n",
    "        ab = lab[..., 1:3].reshape(-1, 2)\n",
    "\n",
    "        snapped = snap_to_grid(ab)\n",
    "        idxs = grid_index(snapped)\n",
    "        # count hits (frequency-weighted)\n",
    "        counts += np.bincount(idxs, minlength=G)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            observed = int((counts > 0).sum())\n",
    "            print(f\"  prune {j}/{prune_n} | observed grid points: {observed}/{G}\")\n",
    "\n",
    "    observed_mask = counts > 0\n",
    "    obs_points = GRID_POINTS[observed_mask]\n",
    "    obs_counts = counts[observed_mask].astype(np.float64)\n",
    "\n",
    "    print(\"Observed grid points:\", obs_points.shape[0])\n",
    "    k_chosen= np.minimum( K, obs_points.shape[0] )\n",
    "    print(\"Using K =\", k_chosen)\n",
    "    # Weighted k-means on observed grid points\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=k_chosen,\n",
    "        random_state=SEED,\n",
    "        batch_size=2048,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        init_size=20000,\n",
    "        reassignment_ratio=0.01,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        kmeans.fit(obs_points, sample_weight=obs_counts)\n",
    "        print(\"KMeans fit used sample_weight.\")\n",
    "    except TypeError:\n",
    "        print(\"KMeans sample_weight not supported; falling back to repetition.\")\n",
    "        rep = np.clip((obs_counts / obs_counts.mean()).round().astype(np.int32), 1, 200)\n",
    "        rep_points = np.repeat(obs_points, rep, axis=0)\n",
    "        kmeans.fit(rep_points)\n",
    "\n",
    "    centers = kmeans.cluster_centers_.astype(np.float32)\n",
    "    np.save(CENTERS_NPY, centers)\n",
    "    meta = {\n",
    "        \"dataset\": \"Food101\",\n",
    "        \"K\": int(k_chosen),\n",
    "        \"ab_min\": AB_MIN,\n",
    "        \"ab_max\": AB_MAX,\n",
    "        \"grid_step\": GRID_STEP,\n",
    "        \"prune_images\": prune_n,\n",
    "        \"resize_policy\": \"short_side=256, center_crop=224\",\n",
    "        \"nearest_grid\": True,\n",
    "        \"weighted_kmeans\": True,\n",
    "        \"sigma_soft\": SIGMA_SOFT,\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    "    CENTERS_META = ART_DIR / f\"ab_centers_k{K}_meta.json\"\n",
    "    CENTERS_META.write_text(json.dumps(meta, indent=2))\n",
    "    print(\"Saved:\", CENTERS_NPY)\n",
    "    print(\"Saved:\", CENTERS_META)\n",
    "    return centers, k_chosen\n",
    "\n",
    "centers, k_chosen = build_centers_from_food101()\n",
    "print(\"centers shape:\", centers.shape)\n",
    "print(\"centers range:\",\n",
    "      \"a:\", float(centers[:,0].min()), float(centers[:,0].max()),\n",
    "      \"| b:\", float(centers[:,1].min()), float(centers[:,1].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6: Compute rebalancing weights (soft prior + smoothing + lambda mix)\n",
    "# =========================\n",
    "WEIGHTS_NPY  = ART_DIR / f\"ab_weights_k{k_chosen}.npy\"\n",
    "WEIGHTS_META = ART_DIR / f\"ab_weights_k{k_chosen}_meta.json\"\n",
    "\n",
    "nn_centers_5 = NearestNeighbors(n_neighbors=min(SOFT_KNN, k_chosen), algorithm=\"auto\").fit(centers)\n",
    "nn_centers_smooth = NearestNeighbors(n_neighbors=min(60, k_chosen), algorithm=\"auto\").fit(centers)\n",
    "\n",
    "def soft_encode_ab(ab_hw2: np.ndarray, sigma=SIGMA_SOFT):\n",
    "    \"\"\"\n",
    "    ab_hw2: (H*W,2)\n",
    "    returns:\n",
    "      idx: (H*W,5) int64\n",
    "      w  : (H*W,5) float32, rows sum to 1\n",
    "    \"\"\"\n",
    "    dists, idx = nn_centers_5.kneighbors(ab_hw2, return_distance=True)  # dists: (N,5)\n",
    "    w = np.exp(-(dists**2) / (2.0 * sigma * sigma)).astype(np.float32)\n",
    "    w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    return idx.astype(np.int64), w\n",
    "\n",
    "def smooth_prior(p: np.ndarray, sigma=SIGMA_SMOOTH):\n",
    "    \"\"\"\n",
    "    Smooth prior across centers using Gaussian weights in ab space.\n",
    "    Approximated with kNN on centers (60 neighbors).\n",
    "    \"\"\"\n",
    "    # For each center q', distribute p[q'] to its neighbors\n",
    "    dists, nbrs = nn_centers_smooth.kneighbors(centers, return_distance=True)  # both (K,Kn)\n",
    "    Kn = nbrs.shape[1]\n",
    "    W = np.exp(-(dists**2) / (2.0 * sigma * sigma)).astype(np.float64)  # (K,Kn)\n",
    "    W /= (W.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    p_s = np.zeros_like(p, dtype=np.float64)\n",
    "    for q in range(k_chosen):\n",
    "        p_s[nbrs[q]] += p[q] * W[q]\n",
    "    p_s = p_s / (p_s.sum() + 1e-12)\n",
    "    return p_s.astype(np.float64)\n",
    "\n",
    "def compute_rebalancing_weights():\n",
    "    if WEIGHTS_NPY.exists():\n",
    "        print(\"Weights already exist:\", WEIGHTS_NPY)\n",
    "        return np.load(WEIGHTS_NPY).astype(np.float32)\n",
    "\n",
    "    # Choose indices for prior computation\n",
    "    prior_ids = trn_idx if PRIOR_USE_ALL_TRAIN else trn_idx[:PRUNE_IMAGES]\n",
    "    print(\"Computing prior from images:\", len(prior_ids))\n",
    "\n",
    "    hist = np.zeros(k_chosen, dtype=np.float64)\n",
    "\n",
    "    # Image-level batching for speed (still CPU work)\n",
    "    for start in range(0, len(prior_ids), PRIOR_BATCH_IMAGES):\n",
    "        batch_ids = prior_ids[start:start+PRIOR_BATCH_IMAGES]\n",
    "        for i in batch_ids:\n",
    "            img, _ = train_base[i]\n",
    "            img = prior_tf(img)  # center crop, per our lock\n",
    "            rgb01 = pil_to_rgb01(img)\n",
    "            lab = clamp_ab(rgb01_to_lab(rgb01))\n",
    "            ab = lab[..., 1:3].reshape(-1, 2)\n",
    "\n",
    "            idx5, w5 = soft_encode_ab(ab, sigma=SIGMA_SOFT)\n",
    "            # accumulate soft counts\n",
    "            # hist[q] += sum over pixels of weights assigned to q\n",
    "            flat_idx = idx5.reshape(-1)\n",
    "            flat_w   = w5.reshape(-1)\n",
    "            hist += np.bincount(flat_idx, weights=flat_w, minlength=k_chosen)\n",
    "\n",
    "        if (start // PRIOR_BATCH_IMAGES) % 20 == 0:\n",
    "            done = min(start + PRIOR_BATCH_IMAGES, len(prior_ids))\n",
    "            print(f\"  prior progress: {done}/{len(prior_ids)}\")\n",
    "\n",
    "    p = hist / (hist.sum() + 1e-12)\n",
    "    p_s = smooth_prior(p, sigma=SIGMA_SMOOTH)\n",
    "\n",
    "    p_tilde = (1.0 - LAMBDA_UNIFORM) * p_s + LAMBDA_UNIFORM * (1.0 / k_chosen)\n",
    "    w = 1.0 / (p_tilde + 1e-12)\n",
    "    w = w / w.mean()\n",
    "\n",
    "\n",
    "\n",
    "    w = w.astype(np.float32)\n",
    "    np.save(WEIGHTS_NPY, w)\n",
    "\n",
    "    meta = {\n",
    "    \"dataset\": \"Food101\",\n",
    "    \"K\": int(k_chosen),\n",
    "    \"lambda_uniform\": float(LAMBDA_UNIFORM),\n",
    "    \"sigma_soft\": float(SIGMA_SOFT),\n",
    "    \"sigma_smooth\": float(SIGMA_SMOOTH),\n",
    "    \"prior_soft_counts\": True,\n",
    "    \"prior_transform\": \"short_side=256, center_crop=224\",\n",
    "    \"annealed_T\": float(ANNEAL_T),\n",
    "    \"centers_file\": str(CENTERS_NPY.resolve()),\n",
    "    \"seed\": int(SEED),\n",
    "    \"prior_images_used\": int(len(prior_ids)),\n",
    "    }\n",
    "    WEIGHTS_META.write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    print(\"Saved:\", WEIGHTS_NPY)\n",
    "    print(\"Saved:\", WEIGHTS_META)\n",
    "    print(\"sum(p):\", p.sum())\n",
    "    print(\"mean(w):\", w.mean(), \"min/max:\", w.min(), w.max())\n",
    "    print(\"top-10 weight bins:\", np.sort(w)[-10:])\n",
    "\n",
    "    return w\n",
    "\n",
    "ab_weights = compute_rebalancing_weights()\n",
    "print(\"weights:\", ab_weights.shape, \"min/max:\", float(ab_weights.min()), float(ab_weights.max()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
